{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNx+ojVNMX2RRv5JkJsSSF0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rxUO7ClbglT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "dci-84AGbrAW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJRjH6kBbrZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "73grnqA8br5a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jXp-SIEbsOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Project Setup & Obtaining Dataset"
      ],
      "metadata": {
        "id": "f0k_ALtzjt0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "print(\"TensorFlow GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"PyTorch GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "# If using PyTorch, print GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"PyTorch GPU name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "RAyBe-2WdB76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# install and update Hugging Face Transformers, Datasets, Accelerate, Evaluate\n",
        "# Also ensure fsspec and huggingface_hub are up-to-date to resolve common loading issues\n",
        "!pip install -U transformers datasets accelerate evaluate huggingface_hub fsspec sentencepiece -q"
      ],
      "metadata": {
        "id": "Ib9VUaw0dDpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the ADE-Corpus-V2 classification dataset\n",
        "# this dataset contains sentences labeled as 0 (not ADE) or 1 (ADE)\n",
        "dataset = load_dataset(\"SetFit/ade_corpus_v2_classification\")\n",
        "\n",
        "print(\"\\nDataset loaded successfully!\")\n",
        "print(dataset)\n",
        "print(\"\\nKeys in the dataset object:\", dataset.keys())"
      ],
      "metadata": {
        "id": "H_sTj28MdLqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training split\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "print(\"\\n--- Training Dataset Sample ---\")\n",
        "print(train_dataset[0]) # Print the first example\n",
        "print(train_dataset[1]) # Print the second example\n",
        "\n",
        "print(\"\\n--- Test Dataset Sample ---\")\n",
        "print(test_dataset[0]) # Print the first example\n",
        "\n",
        "print(\"\\nFeatures (columns) available:\", train_dataset.column_names)\n",
        "print(\"Label mapping (if available):\", train_dataset.features['label'])"
      ],
      "metadata": {
        "id": "DkPteJ8odM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Exploratory Data Analysis & Initial Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "IVxRVJOXdOnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert Hugging Face Datasets to Pandas DataFrames for easier EDA\n",
        "# initial exploration,\n",
        "train_df = dataset['train'].to_pandas()\n",
        "test_df = dataset['test'].to_pandas()\n",
        "\n",
        "print(f\"Train dataset size: {len(train_df)} rows\")\n",
        "print(f\"Test dataset size: {len(test_df)} rows\")\n",
        "\n",
        "print(\"\\n--- Training Data Class Distribution ---\")\n",
        "train_class_counts = train_df['label_text'].value_counts()\n",
        "print(train_class_counts)\n",
        "\n",
        "print(\"\\n--- Test Data Class Distribution ---\")\n",
        "test_class_counts = test_df['label_text'].value_counts()\n",
        "print(test_class_counts)\n",
        "\n",
        "# Visualize class distribution for training set\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x=train_class_counts.index, y=train_class_counts.values, palette=\"viridis\")\n",
        "plt.title('Training Data Class Distribution (ADE vs. Non-ADE)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n",
        "\n",
        "# Visualize class distribution for test set\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x=test_class_counts.index, y=test_class_counts.values, palette=\"plasma\")\n",
        "plt.title('Test Data Class Distribution (ADE vs. Non-ADE)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BDcDZfZodO9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate sentence lengths\n",
        "train_df['text_length'] = train_df['text'].apply(len)\n",
        "test_df['text_length'] = test_df['text'].apply(len)\n",
        "\n",
        "print(\"\\n--- Training Data Sentence Length Statistics (Characters) ---\")\n",
        "print(train_df['text_length'].describe())\n",
        "\n",
        "print(\"\\n--- Test Data Sentence Length Statistics (Characters) ---\")\n",
        "print(test_df['text_length'].describe())\n",
        "\n",
        "# Visualize sentence length distribution for training set\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['text_length'], bins=50, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Sentence Lengths in Training Data')\n",
        "plt.xlabel('Sentence Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Visualize sentence length distribution for test set\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(test_df['text_length'], bins=50, kde=True, color='lightcoral')\n",
        "plt.title('Distribution of Sentence Lengths in Test Data')\n",
        "plt.xlabel('Sentence Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_OLtHFj-juGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Applies basic text cleaning: lowercasing, removing extra whitespace,\n",
        "    and removing non-alphanumeric characters (keeping spaces).\n",
        "    \"\"\"\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Remove special characters, keep letters, numbers, spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Replace multiple spaces with single space and strip leading/trailing\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the 'text' column in both train and test dataframes\n",
        "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
        "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "print(\"\\n--- Original vs. Cleaned Text Examples (Training Data) ---\")\n",
        "for i in range(5):\n",
        "    print(f\"Original: {train_df['text'].iloc[i]}\")\n",
        "    print(f\"Cleaned:  {train_df['cleaned_text'].iloc[i]}\\n\")\n",
        "\n",
        "# Store the dataframes back into the dataset object, or simply use train_df/test_df for next phase\n",
        "# For simplicity, keep working with train_df and test_df for now,\n",
        "# and convert back to Hugging Face Dataset format when needed\n",
        "\n",
        "# to update the original 'dataset' object:\n",
        "# from datasets import Dataset\n",
        "# dataset['train'] = Dataset.from_pandas(train_df)\n",
        "# dataset['test'] = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "M8KP0rG8l0UU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}