{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0NhNs429M62ejGy1VpB5R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adverse Drug Event (ADE) Classification: AI for Drug Safety\n",
        "\n",
        "An NLP project leveraging **Transformer models (BERT)** to automatically classify sentences describing **Adverse Drug Events (ADEs)** in healthcare text. This showcases a full ML workflow, from data analysis to advanced model evaluation, providing a robust solution for enhancing drug safety monitoring."
      ],
      "metadata": {
        "id": "KgcJiM8EWnEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "dci-84AGbrAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Features & Project Impact\n",
        "\n",
        "* **Precision ADE Detection:** Model classifies sentences as either **\"Related\"** (contains an ADE) or **\"Not-Related\"** (does not contain an ADE).\n",
        "* **Significant Performance Improvement:** Achieved a **macro F1-score of 0.94** on the test set, with a critical **0.92 F1-score for the 'Related' (ADE) class**. This is a substantial leap from the traditional ML baseline's 0.78 F1-score for ADEs.\n",
        "* **Reduced Critical Errors:** The Transformer model drastically reduced **False Negatives** (missed ADEs) by 62% (from 260 to 99) and **False Positives** (false alarms) by 63% (from 527 to 193) compared to the baseline.\n"
      ],
      "metadata": {
        "id": "lkeFRjNqWtsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Technical Highlights\n",
        "\n",
        "* **Dataset:** `ADE-Corpus-V2` from Hugging Face Hub (17,637 training, 5,879 test sentences), identified as having a significant **~2.5:1 class imbalance** (Not-Related:Related).\n",
        "* **Preprocessing:** Basic text cleaning (lowercasing, punctuation removal) and Transformer-specific tokenization (subword units, `[CLS]`, `[SEP]`, attention masks, padding).\n",
        "* **Baseline Model:** Logistic Regression trained on Bag-of-Words features, establishing a F1-score baseline of **0.78 for the 'Related' class**.\n",
        "* **Advanced Model:** Fine-tuned **`bert-base-uncased` (110M parameters)** using the Hugging Face `transformers` library and `Trainer` API.\n",
        "* **Evaluation Metrics:** Comprehensive use of Precision, Recall, F1-score, and Confusion Matrices for both overall and per-class performance.\n",
        "* **Model Persistence:** Implemented saving and loading of the 417 MB trained model and tokenizer to Google Drive, ensuring reusability without re-training. The model is also ready for upload to Hugging Face Hub.\n"
      ],
      "metadata": {
        "id": "jHcGHFpJWxfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Key Learnings & Challenges\n",
        "\n",
        "* **Full ML Lifecycle Mastery:** Gained hands-on experience across the entire ML project pipeline: data acquisition, rigorous EDA, traditional ML baselines, advanced deep learning model training, and in-depth evaluation.\n",
        "* **Navigating Class Imbalance:** Effectively managed and accounted for dataset imbalance using appropriate strategies and evaluation metrics.\n",
        "* **Critical Error Analysis:** Performed detailed error analysis, including pinpointing and dissecting a specific instance where the model (despite high overall performance) made a **highly confident False Negative** prediction for a clear ADE sentence.\n",
        "* **Practical MLOps Foundations:** Learned essential practices for model persistence, understanding of deployment modalities, and the crucial need for post-deployment monitoring.\n",
        "* **Library Version Management:** Successfully navigated and resolved multiple dependency and version conflicts within the `transformers` ecosystem.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "73grnqA8br5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Run This Project\n",
        "\n",
        "To run this project and explore the code:\n",
        "\n",
        "1.  **Clone the Repository:**\n",
        "    ```bash\n",
        "    git clone [https://github.com/your-username/your-repo-name.git](https://github.com/your-username/your-repo-name.git)\n",
        "    cd your-repo-name\n",
        "    ```\n",
        "2.  **Open in Google Colab:** Go to `colab.research.google.com`, then `File > Open notebook > GitHub`, and paste your repository URL.\n",
        "3.  **Set Up Runtime:** Change Colab runtime to `GPU` (`Runtime > Change runtime type`).\n",
        "4.  **Mount Google Drive:** (If loading the model from Drive)\n",
        "    ```python\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ```\n",
        "5.  **Install Libraries:**\n",
        "    ```bash\n",
        "    !pip install -r requirements.txt -q\n",
        "    # IMPORTANT: If you hit dependency errors, restart runtime AFTER install, then remount Drive and reinstall.\n",
        "    ```\n",
        "6.  **Run the Notebook:**\n",
        "    * **Option A: Re-train Model:** Run all cells sequentially (approx. 1.5 hours).\n",
        "    * **Option B: Load Pre-trained Model:** Update the `load_directory` variable to your Google Drive path (e.g., `\"/content/drive/MyDrive/ml_models/my_ade_bert_model\"`) or use the Hugging Face Hub path (e.g., `\"your-huggingface-username/your-repo-name-on-hub\"`) and run the model loading and evaluation cells.\n",
        "\n",
        "*Note: The trained model (417MB) is not directly included in this repository to keep it lightweight. Please use the instructions above to load it from Google Drive or Hugging Face Hub.*\n",
        "\n",
        "---\n",
        "\n",
        "## Technologies Used\n",
        "\n",
        "Python, PyTorch, Hugging Face (transformers, datasets, accelerate, evaluate), scikit-learn, pandas, matplotlib, seaborn, Google Colaboratory, Google Drive.\n",
        "\n"
      ],
      "metadata": {
        "id": "QQ-aPiv5W-vU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "zEM3yKVti9OZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the already trained and saved model through Google Drive"
      ],
      "metadata": {
        "id": "jLRU7ltti4hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GmTcK52r1s8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install Hugging Face Transformers and Datasets libraries\n",
        "# -U to get up-to-date versions, and include fsspec/gcsfs for dependency stability\n",
        "!pip install -U transformers datasets accelerate evaluate huggingface_hub fsspec gcsfs sentencepiece -q"
      ],
      "metadata": {
        "id": "wk_3ssdG1vmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Saved Model and Tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# define exact directory path where you saved your model in Google Drive\n",
        "# Make sure this matches the path you used when saving (from the 'ls -l' command confirmation)\n",
        "load_directory = \"/content/drive/MyDrive/ml_models/my_ade_bert_model\" # <--- YOUR EXACT PATH HERE\n",
        "\n",
        "# load tokenizer\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
        "\n",
        "# load the model\n",
        "# automatically load the configuration and weights from the saved files\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(load_directory)\n",
        "\n",
        "print(f\"Model and tokenizer loaded successfully from: {load_directory}\")\n",
        "\n",
        "# move the model to GPU for faster inference\n",
        "if torch.cuda.is_available():\n",
        "    loaded_model.to('cuda')\n",
        "    print(\"Model moved to GPU.\")"
      ],
      "metadata": {
        "id": "VoBZL4sL1zqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testthe Loaded Model with a Sample Sentence\n",
        "# using the loaded_tokenizer and loaded_model\n",
        "\n",
        "sample_text_ade = \"Patient developed severe nausea and vomiting after starting new chemotherapy drug.\"\n",
        "sample_text_not_ade = \"Patient was discharged after successful knee surgery.\"\n",
        "\n",
        "# tokenize the input text\n",
        "inputs_ade = loaded_tokenizer(sample_text_ade, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs_not_ade = loaded_tokenizer(sample_text_not_ade, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# if model on GPU, move inputs to GPU\n",
        "if torch.cuda.is_available():\n",
        "    inputs_ade = {k: v.to('cuda') for k, v in inputs_ade.items()}\n",
        "    inputs_not_ade = {k: v.to('cuda') for k, v in inputs_not_ade.items()}\n",
        "\n",
        "# get model predictions (logits)\n",
        "loaded_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_ade = loaded_model(**inputs_ade)\n",
        "    outputs_not_ade = loaded_model(**inputs_not_ade)\n",
        "\n",
        "# get predicted class (0 or 1)\n",
        "# logits are usually in outputs.logits\n",
        "predicted_class_ade = torch.argmax(outputs_ade.logits, dim=1).item()\n",
        "predicted_class_not_ade = torch.argmax(outputs_not_ade.logits, dim=1).item()\n",
        "\n",
        "# map the class ID back to labels (0: Not-Related, 1: Related)\n",
        "labels_map = {0: 'Not-Related', 1: 'Related'}\n",
        "\n",
        "print(f\"\\nSample sentence: '{sample_text_ade}'\")\n",
        "print(f\"Predicted class: {labels_map[predicted_class_ade]}\")\n",
        "\n",
        "print(f\"\\nSample sentence: '{sample_text_not_ade}'\")\n",
        "print(f\"Predicted class: {labels_map[predicted_class_not_ade]}\")"
      ],
      "metadata": {
        "id": "WmvJTPFE11eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for confidence score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# assuming outputs_ade contains the logits for the ADE sample\n",
        "logits_ade = outputs_ade.logits\n",
        "probabilities_ade = F.softmax(logits_ade, dim=1)\n",
        "\n",
        "print(f\"\\nLogits for ADE sample: {logits_ade}\")\n",
        "print(f\"Probabilities for ADE sample (Not-Related, Related): {probabilities_ade}\")"
      ],
      "metadata": {
        "id": "09tH86P7wG5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Project Setup & Obtaining Dataset"
      ],
      "metadata": {
        "id": "f0k_ALtzjt0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "print(\"TensorFlow GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"PyTorch GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "# If using PyTorch, print GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"PyTorch GPU name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "RAyBe-2WdB76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# install and update Hugging Face Transformers, Datasets, Accelerate, Evaluate\n",
        "# Also ensure fsspec and huggingface_hub are up-to-date to resolve common loading issues\n",
        "!pip install -U transformers datasets accelerate evaluate huggingface_hub fsspec sentencepiece -q"
      ],
      "metadata": {
        "id": "Ib9VUaw0dDpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the ADE-Corpus-V2 classification dataset\n",
        "# this dataset contains sentences labeled as 0 (not ADE) or 1 (ADE)\n",
        "dataset = load_dataset(\"SetFit/ade_corpus_v2_classification\")\n",
        "\n",
        "print(\"\\nDataset loaded successfully!\")\n",
        "print(dataset)\n",
        "print(\"\\nKeys in the dataset object:\", dataset.keys())"
      ],
      "metadata": {
        "id": "H_sTj28MdLqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training split\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "print(\"\\n--- Training Dataset Sample ---\")\n",
        "print(train_dataset[0]) # Print the first example\n",
        "print(train_dataset[1]) # Print the second example\n",
        "\n",
        "print(\"\\n--- Test Dataset Sample ---\")\n",
        "print(test_dataset[0]) # Print the first example\n",
        "\n",
        "print(\"\\nFeatures (columns) available:\", train_dataset.column_names)\n",
        "print(\"Label mapping (if available):\", train_dataset.features['label'])"
      ],
      "metadata": {
        "id": "DkPteJ8odM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Exploratory Data Analysis & Initial Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "IVxRVJOXdOnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert Hugging Face Datasets to Pandas DataFrames for easier EDA\n",
        "# initial exploration,\n",
        "train_df = dataset['train'].to_pandas()\n",
        "test_df = dataset['test'].to_pandas()\n",
        "\n",
        "print(f\"Train dataset size: {len(train_df)} rows\")\n",
        "print(f\"Test dataset size: {len(test_df)} rows\")\n",
        "\n",
        "print(\"\\n--- Training Data Class Distribution ---\")\n",
        "train_class_counts = train_df['label_text'].value_counts()\n",
        "print(train_class_counts)\n",
        "\n",
        "print(\"\\n--- Test Data Class Distribution ---\")\n",
        "test_class_counts = test_df['label_text'].value_counts()\n",
        "print(test_class_counts)\n",
        "\n",
        "# Visualize class distribution for training set\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x=train_class_counts.index, y=train_class_counts.values, palette=\"viridis\")\n",
        "plt.title('Training Data Class Distribution (ADE vs. Non-ADE)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n",
        "\n",
        "# Visualize class distribution for test set\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x=test_class_counts.index, y=test_class_counts.values, palette=\"plasma\")\n",
        "plt.title('Test Data Class Distribution (ADE vs. Non-ADE)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BDcDZfZodO9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate sentence lengths\n",
        "train_df['text_length'] = train_df['text'].apply(len)\n",
        "test_df['text_length'] = test_df['text'].apply(len)\n",
        "\n",
        "print(\"\\n--- Training Data Sentence Length Statistics (Characters) ---\")\n",
        "print(train_df['text_length'].describe())\n",
        "\n",
        "print(\"\\n--- Test Data Sentence Length Statistics (Characters) ---\")\n",
        "print(test_df['text_length'].describe())\n",
        "\n",
        "# Visualize sentence length distribution for training set\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['text_length'], bins=50, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Sentence Lengths in Training Data')\n",
        "plt.xlabel('Sentence Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Visualize sentence length distribution for test set\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(test_df['text_length'], bins=50, kde=True, color='lightcoral')\n",
        "plt.title('Distribution of Sentence Lengths in Test Data')\n",
        "plt.xlabel('Sentence Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_OLtHFj-juGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Applies basic text cleaning: lowercasing, removing extra whitespace,\n",
        "    and removing non-alphanumeric characters (keeping spaces).\n",
        "    \"\"\"\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Remove special characters, keep letters, numbers, spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Replace multiple spaces with single space and strip leading/trailing\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the 'text' column in both train and test dataframes\n",
        "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
        "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "print(\"\\n--- Original vs. Cleaned Text Examples (Training Data) ---\")\n",
        "for i in range(5):\n",
        "    print(f\"Original: {train_df['text'].iloc[i]}\")\n",
        "    print(f\"Cleaned:  {train_df['cleaned_text'].iloc[i]}\\n\")\n",
        "\n",
        "# Store the dataframes back into the dataset object, or simply use train_df/test_df for next phase\n",
        "# For simplicity, keep working with train_df and test_df for now,\n",
        "# and convert back to Hugging Face Dataset format when needed\n",
        "\n",
        "# to update the original 'dataset' object:\n",
        "# from datasets import Dataset\n",
        "# dataset['train'] = Dataset.from_pandas(train_df)\n",
        "# dataset['test'] = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "M8KP0rG8l0UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Baseline Model Development (ML & Feature Engineering)"
      ],
      "metadata": {
        "id": "iqhsD-6lst3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer - counts the occurrences of each word\n",
        "# max_features limits the number of unique words (vocabulary size) to consider,\n",
        "# which can help manage memory and focus on most frequent words.\n",
        "# min_df ignores words that appear in too few documents (e.g., typos, very rare words)\n",
        "# max_df ignores words that appear in too many documents (e.g., common words that aren't stop words)\n",
        "count_vectorizer = CountVectorizer(max_features=5000, min_df=5, max_df=0.9)\n",
        "\n",
        "# fit the vectorizer on the training data's cleaned text and transform both train and test data\n",
        "X_train_bow = count_vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "X_test_bow = count_vectorizer.transform(test_df['cleaned_text'])\n",
        "\n",
        "# get labels (target variable)\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n",
        "\n",
        "print(f\"Shape of X_train_bow: {X_train_bow.shape}\")\n",
        "print(f\"Shape of X_test_bow: {X_test_bow.shape}\")\n",
        "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")"
      ],
      "metadata": {
        "id": "o1M5DRW5s2Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model, with CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "# max_iter increased to ensure convergence for larger datasets\n",
        "# solver='liblinear' good for smaller datasets and L1/L2 regularization\n",
        "# class_weight='balanced' CRUCIAL here because of our class imbalance.\n",
        "# It automatically adjusts weights inversely proportional to class frequencies.\n",
        "log_reg_model_bow = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "log_reg_model_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_bow = log_reg_model_bow.predict(X_test_bow)\n",
        "\n",
        "print(\"\\n--- Logistic Regression (Bag-of-Words) Performance ---\")\n",
        "\n",
        "# Classification Report\n",
        "# This provides Precision, Recall, F1-score, and Support for each class.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_bow, target_names=['Not-Related', 'Related']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_bow = confusion_matrix(y_test, y_pred_bow, labels=log_reg_model_bow.classes_)\n",
        "disp_bow = ConfusionMatrixDisplay(confusion_matrix=cm_bow, display_labels=['Not-Related', 'Related'])\n",
        "disp_bow.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix (Bag-of-Words)')\n",
        "plt.show()\n",
        "\n",
        "# raw confusion matrix\n",
        "print(\"\\nRaw Confusion Matrix:\\n\", cm_bow)"
      ],
      "metadata": {
        "id": "eAjFSknWwYnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer - Term Frequency-Inverse Document Frequency is\n",
        "# another common technique. It not only counts word occurrences but also gives\n",
        "# more weight to words that are rare across the entire corpus but frequent in a\n",
        "# specific document, it helps highlight words that are more distinctive.\n",
        "# Same parameters as CountVectorizer for consistency\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.9)\n",
        "\n",
        "# Fit & transform training data, transform test data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text'])\n",
        "\n",
        "print(f\"Shape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
        "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape}\")"
      ],
      "metadata": {
        "id": "0sMlwZSRyMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model, using TF-IDF features.\n",
        "\n",
        "# Initialize Logistic Regression model with class_weight='balanced'\n",
        "log_reg_model_tfidf = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "log_reg_model_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_tfidf = log_reg_model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"\\n--- Logistic Regression (TF-IDF) Performance ---\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tfidf, target_names=['Not-Related', 'Related']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_tfidf = confusion_matrix(y_test, y_pred_tfidf, labels=log_reg_model_tfidf.classes_)\n",
        "disp_tfidf = ConfusionMatrixDisplay(confusion_matrix=cm_tfidf, display_labels=['Not-Related', 'Related'])\n",
        "disp_tfidf.plot(cmap=plt.cm.Greens)\n",
        "plt.title('Confusion Matrix (TF-IDF)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRaw Confusion Matrix:\\n\", cm_tfidf)"
      ],
      "metadata": {
        "id": "DkoJQVDby7rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Deep Learning Model (Transformer)"
      ],
      "metadata": {
        "id": "tttbJAe_5Twq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# for general-purpose model, starting with 'bert-base-uncased'\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Tokenizer for {model_name} loaded.\")\n",
        "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Test tokenizer on a sample sentence\n",
        "sample_text = \"The patient experienced severe headaches after taking the medication.\"\n",
        "tokenized_output = tokenizer(sample_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "print(\"\\nSample Tokenization Output:\")\n",
        "print(f\"Input IDs: {tokenized_output['input_ids']}\")\n",
        "print(f\"Attention Mask: {tokenized_output['attention_mask']}\")\n",
        "print(f\"Decoded (for understanding): {tokenizer.decode(tokenized_output['input_ids'][0])}\")"
      ],
      "metadata": {
        "id": "8HBDoYGi0Tt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The label column should be named 'label' (numerical 0 or 1)\n",
        "# The text column should be named 'text' (original text, as tokenizer handles cleaning)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Truncate to the maximum input length of the model (often 512)\n",
        "    # Based on EDA, max sentence length was 742 chars. 512 tokens often covers more characters.\n",
        "    # We will use padding='max_length' to pad to the max length, and truncation=True for longer texts.\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenizer to the entire dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Rename the 'label_text' column to 'labels' and remove unnecessary columns\n",
        "# Transformers Trainer expects the target column to be named 'labels'\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label_text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Set the format to PyTorch tensors (important for training)\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"\\nTokenized Datasets Overview:\")\n",
        "print(tokenized_datasets)\n",
        "print(\"\\nSample of tokenized_datasets['train'][0]:\")\n",
        "print(tokenized_datasets['train'][0])"
      ],
      "metadata": {
        "id": "NEjyJTJEsrX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# load the pre-trained model for sequence classification\n",
        "# num_labels=2 for binary classification (ADE vs. Non-ADE)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# function to compute metrics\n",
        "# for evaluating model during training and after.\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = evaluate.load(\"f1\")\n",
        "    # metric_precision = evaluate.load(\"precision\")\n",
        "    # metric_recall = evaluate.load(\"recall\")\n",
        "    # metric_accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Compute F1-score (macro average is good for imbalanced datasets)\n",
        "    f1_score = metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "\n",
        "    # return other metrics as a dictionary\n",
        "    # return {\n",
        "    #     \"accuracy\": metric_accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
        "    #     \"precision\": metric_precision.compute(predictions=predictions, references=labels, average=\"macro\")['precision'],\n",
        "    #     \"recall\": metric_recall.compute(predictions=predictions, references=labels, average=\"macro\")['recall'],\n",
        "    #     \"f1\": f1_score['f1']\n",
        "    # }\n",
        "    return f1_score\n",
        "\n",
        "# output_dir: where the model checkpoints and logs will be saved\n",
        "# evaluation_strategy: 'epoch' evaluate at the end of each epoch\n",
        "# num_train_epochs: number of passes over the training data\n",
        "# weight_decay: regularization to prevent overfitting\n",
        "# load_best_model_at_end: load the model with the best validation performance\n",
        "# metric_for_best_model: the metric to monitor for early stopping/best model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16, # adjust based on GPU memory\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3, # typically 2-4 epochs are enough for fine-tuning\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\", # monitor macro F1-score on validation\n",
        "    save_strategy=\"epoch\", # checkpoints at each epoch\n",
        "    report_to=\"none\" # disable logging for simplicity\n",
        ")"
      ],
      "metadata": {
        "id": "HXX8wstQxbd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tf-keras"
      ],
      "metadata": {
        "id": "-Il4VRULK4SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# init the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"], #'test' as our validation set\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# eval the model on the test set after training\n",
        "final_results = trainer.evaluate()\n",
        "print(\"\\nFinal Evaluation Results on Test Set:\")\n",
        "print(final_results)"
      ],
      "metadata": {
        "id": "cIHZHuvPwQBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (Previous code in Cell 15, including trainer.train() and trainer.evaluate()) ...\n",
        "\n",
        "# Define a directory to save your model and tokenizer\n",
        "save_directory = \"./my_ade_bert_model\"\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "# This saves the model's weights, configuration, and the tokenizer vocabulary/config\n",
        "trainer.save_model(save_directory) # The Trainer has a convenient save_model method\n",
        "tokenizer.save_pretrained(save_directory) # Save the tokenizer too\n",
        "\n",
        "print(f\"\\nModel and tokenizer saved to: {save_directory}\")\n",
        "\n",
        "# To verify, list the contents of the saved directory\n",
        "!ls -l {save_directory}"
      ],
      "metadata": {
        "id": "4gvpu2b5atUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: save to a folder named 'ml_models' in your Google Drive\n",
        "save_directory = \"/content/drive/MyDrive/ml_models/my_ade_bert_model\""
      ],
      "metadata": {
        "id": "LXOsJf9Pye9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you mounted Google Drive earlier with:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "save_directory = \"/content/drive/MyDrive/ml_models/my_ade_bert_model\"\n",
        "# Or \"/content/drive/MyDrive/my_ade_bert_model\" if you're saving directly in MyDrive"
      ],
      "metadata": {
        "id": "Ly1MdmLMzt-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a shell command to list the contents of your specified save directory in Drive\n",
        "# Replace '/content/drive/MyDrive/ml_models/my_ade_bert_model' with your exact save path\n",
        "!ls -l \"/content/drive/MyDrive/ml_models/my_ade_bert_model\""
      ],
      "metadata": {
        "id": "cl4uU3zm0D9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory on Google Drive\n",
        "google_drive_save_directory = \"/content/drive/MyDrive/ml_models/my_ade_bert_model\"\n",
        "\n",
        "# Create the directory if it doesn't exist (important!)\n",
        "import os\n",
        "os.makedirs(google_drive_save_directory, exist_ok=True)\n",
        "\n",
        "# Save the fine-tuned model and tokenizer to Google Drive\n",
        "trainer.save_model(google_drive_save_directory)\n",
        "tokenizer.save_pretrained(google_drive_save_directory)\n",
        "\n",
        "print(f\"\\nModel and tokenizer saved to Google Drive: {google_drive_save_directory}\")\n",
        "\n",
        "# Confirm it's there\n",
        "!ls -l {google_drive_save_directory}"
      ],
      "metadata": {
        "id": "dploNw3-1LWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of tokenization and mapping of input ID to the actual words (tokens)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "sample_text = \"The patient experienced severe headaches after taking the medication.\"\n",
        "\n",
        "# get list of string tokens (before adding [CLS]/[SEP] and converting to IDs)\n",
        "tokens_list = tokenizer.tokenize(sample_text)\n",
        "\n",
        "print(f\"List of tokens: {tokens_list}\")\n",
        "print(f\"Number of tokens in list: {len(tokens_list)}\")\n",
        "\n",
        "# get the input_ids with special tokens and padding\n",
        "tokenized_output = tokenizer(sample_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "input_ids = tokenized_output['input_ids'][0].tolist() #Python list for easier viewing\n",
        "\n",
        "print(f\"\\nRaw Input IDs tensor: {input_ids}\")\n",
        "print(f\"Number of Input IDs: {len(input_ids)}\")\n",
        "\n",
        "# decode the input_ids to confirm\n",
        "decoded_text = tokenizer.decode(input_ids)\n",
        "print(f\"Decoded text from Input IDs: {decoded_text}\")"
      ],
      "metadata": {
        "id": "GW9olx3xUhLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model Evaluation, Optimization & Analysis"
      ],
      "metadata": {
        "id": "17yGl0NGXj0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n--- Detailed Evaluation of Fine-tuned Transformer Model ---\")\n",
        "\n",
        "# get predictions on the test set using the best model (loaded by load_best_model_at_end=True)\n",
        "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
        "\n",
        "# predictions object contains logits (raw model outputs) and labels\n",
        "logits = predictions.predictions\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# convert logits to class predictions (0 or 1)\n",
        "y_pred_transformer = np.argmax(logits, axis=-1)\n",
        "y_test_transformer = labels # The true labels from the test set\n",
        "\n",
        "# get the class names for the report (assuming 0: Not-Related, 1: Related)\n",
        "target_names = ['Not-Related', 'Related']\n",
        "\n",
        "# classification report\n",
        "print(\"\\nClassification Report (Transformer Model):\")\n",
        "print(classification_report(y_test_transformer, y_pred_transformer, target_names=target_names))\n",
        "\n",
        "# confusion matrix\n",
        "cm_transformer = confusion_matrix(y_test_transformer, y_pred_transformer, labels=[0, 1]) # Explicitly define labels\n",
        "disp_transformer = ConfusionMatrixDisplay(confusion_matrix=cm_transformer, display_labels=target_names)\n",
        "disp_transformer.plot(cmap=plt.cm.Purples) # Using a different colormap for distinction\n",
        "plt.title('Confusion Matrix (Transformer Model)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRaw Confusion Matrix (Transformer Model):\\n\", cm_transformer)"
      ],
      "metadata": {
        "id": "K_E8GpWrXoLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}