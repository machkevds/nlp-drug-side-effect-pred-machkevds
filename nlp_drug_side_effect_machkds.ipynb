{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMh31HuB0fbrvffmriM/snP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rxUO7ClbglT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "dci-84AGbrAW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJRjH6kBbrZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "73grnqA8br5a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jXp-SIEbsOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Project Setup & Obtaining Dataset"
      ],
      "metadata": {
        "id": "f0k_ALtzjt0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "print(\"TensorFlow GPU available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"PyTorch GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "# If using PyTorch, print GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"PyTorch GPU name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "RAyBe-2WdB76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# install and update Hugging Face Transformers, Datasets, Accelerate, Evaluate\n",
        "# Also ensure fsspec and huggingface_hub are up-to-date to resolve common loading issues\n",
        "!pip install -U transformers datasets accelerate evaluate huggingface_hub fsspec sentencepiece -q"
      ],
      "metadata": {
        "id": "Ib9VUaw0dDpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the ADE-Corpus-V2 classification dataset\n",
        "# this dataset contains sentences labeled as 0 (not ADE) or 1 (ADE)\n",
        "dataset = load_dataset(\"SetFit/ade_corpus_v2_classification\")\n",
        "\n",
        "print(\"\\nDataset loaded successfully!\")\n",
        "print(dataset)\n",
        "print(\"\\nKeys in the dataset object:\", dataset.keys())"
      ],
      "metadata": {
        "id": "H_sTj28MdLqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training split\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "print(\"\\n--- Training Dataset Sample ---\")\n",
        "print(train_dataset[0]) # Print the first example\n",
        "print(train_dataset[1]) # Print the second example\n",
        "\n",
        "print(\"\\n--- Test Dataset Sample ---\")\n",
        "print(test_dataset[0]) # Print the first example\n",
        "\n",
        "print(\"\\nFeatures (columns) available:\", train_dataset.column_names)\n",
        "print(\"Label mapping (if available):\", train_dataset.features['label'])"
      ],
      "metadata": {
        "id": "DkPteJ8odM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Exploratory Data Analysis & Initial Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "IVxRVJOXdOnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert Hugging Face Datasets to Pandas DataFrames for easier EDA\n",
        "# initial exploration,\n",
        "train_df = dataset['train'].to_pandas()\n",
        "test_df = dataset['test'].to_pandas()\n",
        "\n",
        "print(f\"Train dataset size: {len(train_df)} rows\")\n",
        "print(f\"Test dataset size: {len(test_df)} rows\")\n",
        "\n",
        "print(\"\\n--- Training Data Class Distribution ---\")\n",
        "train_class_counts = train_df['label_text'].value_counts()\n",
        "print(train_class_counts)\n",
        "\n",
        "print(\"\\n--- Test Data Class Distribution ---\")\n",
        "test_class_counts = test_df['label_text'].value_counts()\n",
        "print(test_class_counts)\n",
        "\n",
        "# Visualize class distribution for training set\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x=train_class_counts.index, y=train_class_counts.values, palette=\"viridis\")\n",
        "plt.title('Training Data Class Distribution (ADE vs. Non-ADE)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n",
        "\n",
        "# Visualize class distribution for test set\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(x=test_class_counts.index, y=test_class_counts.values, palette=\"plasma\")\n",
        "plt.title('Test Data Class Distribution (ADE vs. Non-ADE)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BDcDZfZodO9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate sentence lengths\n",
        "train_df['text_length'] = train_df['text'].apply(len)\n",
        "test_df['text_length'] = test_df['text'].apply(len)\n",
        "\n",
        "print(\"\\n--- Training Data Sentence Length Statistics (Characters) ---\")\n",
        "print(train_df['text_length'].describe())\n",
        "\n",
        "print(\"\\n--- Test Data Sentence Length Statistics (Characters) ---\")\n",
        "print(test_df['text_length'].describe())\n",
        "\n",
        "# Visualize sentence length distribution for training set\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['text_length'], bins=50, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Sentence Lengths in Training Data')\n",
        "plt.xlabel('Sentence Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Visualize sentence length distribution for test set\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(test_df['text_length'], bins=50, kde=True, color='lightcoral')\n",
        "plt.title('Distribution of Sentence Lengths in Test Data')\n",
        "plt.xlabel('Sentence Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_OLtHFj-juGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Applies basic text cleaning: lowercasing, removing extra whitespace,\n",
        "    and removing non-alphanumeric characters (keeping spaces).\n",
        "    \"\"\"\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Remove special characters, keep letters, numbers, spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Replace multiple spaces with single space and strip leading/trailing\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the 'text' column in both train and test dataframes\n",
        "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
        "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "print(\"\\n--- Original vs. Cleaned Text Examples (Training Data) ---\")\n",
        "for i in range(5):\n",
        "    print(f\"Original: {train_df['text'].iloc[i]}\")\n",
        "    print(f\"Cleaned:  {train_df['cleaned_text'].iloc[i]}\\n\")\n",
        "\n",
        "# Store the dataframes back into the dataset object, or simply use train_df/test_df for next phase\n",
        "# For simplicity, keep working with train_df and test_df for now,\n",
        "# and convert back to Hugging Face Dataset format when needed\n",
        "\n",
        "# to update the original 'dataset' object:\n",
        "# from datasets import Dataset\n",
        "# dataset['train'] = Dataset.from_pandas(train_df)\n",
        "# dataset['test'] = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "M8KP0rG8l0UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Baseline Model Development (ML & Feature Engineering)"
      ],
      "metadata": {
        "id": "iqhsD-6lst3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer - counts the occurrences of each word\n",
        "# max_features limits the number of unique words (vocabulary size) to consider,\n",
        "# which can help manage memory and focus on most frequent words.\n",
        "# min_df ignores words that appear in too few documents (e.g., typos, very rare words)\n",
        "# max_df ignores words that appear in too many documents (e.g., common words that aren't stop words)\n",
        "count_vectorizer = CountVectorizer(max_features=5000, min_df=5, max_df=0.9)\n",
        "\n",
        "# fit the vectorizer on the training data's cleaned text and transform both train and test data\n",
        "X_train_bow = count_vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "X_test_bow = count_vectorizer.transform(test_df['cleaned_text'])\n",
        "\n",
        "# get labels (target variable)\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n",
        "\n",
        "print(f\"Shape of X_train_bow: {X_train_bow.shape}\")\n",
        "print(f\"Shape of X_test_bow: {X_test_bow.shape}\")\n",
        "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")"
      ],
      "metadata": {
        "id": "o1M5DRW5s2Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model, with CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "# max_iter increased to ensure convergence for larger datasets\n",
        "# solver='liblinear' good for smaller datasets and L1/L2 regularization\n",
        "# class_weight='balanced' CRUCIAL here because of our class imbalance.\n",
        "# It automatically adjusts weights inversely proportional to class frequencies.\n",
        "log_reg_model_bow = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "log_reg_model_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_bow = log_reg_model_bow.predict(X_test_bow)\n",
        "\n",
        "print(\"\\n--- Logistic Regression (Bag-of-Words) Performance ---\")\n",
        "\n",
        "# Classification Report\n",
        "# This provides Precision, Recall, F1-score, and Support for each class.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_bow, target_names=['Not-Related', 'Related']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_bow = confusion_matrix(y_test, y_pred_bow, labels=log_reg_model_bow.classes_)\n",
        "disp_bow = ConfusionMatrixDisplay(confusion_matrix=cm_bow, display_labels=['Not-Related', 'Related'])\n",
        "disp_bow.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix (Bag-of-Words)')\n",
        "plt.show()\n",
        "\n",
        "# raw confusion matrix\n",
        "print(\"\\nRaw Confusion Matrix:\\n\", cm_bow)"
      ],
      "metadata": {
        "id": "eAjFSknWwYnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer - Term Frequency-Inverse Document Frequency is\n",
        "# another common technique. It not only counts word occurrences but also gives\n",
        "# more weight to words that are rare across the entire corpus but frequent in a\n",
        "# specific document, it helps highlight words that are more distinctive.\n",
        "# Same parameters as CountVectorizer for consistency\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.9)\n",
        "\n",
        "# Fit & transform training data, transform test data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text'])\n",
        "\n",
        "print(f\"Shape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
        "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape}\")"
      ],
      "metadata": {
        "id": "0sMlwZSRyMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model, using TF-IDF features.\n",
        "\n",
        "# Initialize Logistic Regression model with class_weight='balanced'\n",
        "log_reg_model_tfidf = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "log_reg_model_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_tfidf = log_reg_model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"\\n--- Logistic Regression (TF-IDF) Performance ---\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tfidf, target_names=['Not-Related', 'Related']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_tfidf = confusion_matrix(y_test, y_pred_tfidf, labels=log_reg_model_tfidf.classes_)\n",
        "disp_tfidf = ConfusionMatrixDisplay(confusion_matrix=cm_tfidf, display_labels=['Not-Related', 'Related'])\n",
        "disp_tfidf.plot(cmap=plt.cm.Greens)\n",
        "plt.title('Confusion Matrix (TF-IDF)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRaw Confusion Matrix:\\n\", cm_tfidf)"
      ],
      "metadata": {
        "id": "DkoJQVDby7rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Deep Learning Model (Transformer)"
      ],
      "metadata": {
        "id": "tttbJAe_5Twq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# for general-purpose model, starting with 'bert-base-uncased'\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Tokenizer for {model_name} loaded.\")\n",
        "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Test tokenizer on a sample sentence\n",
        "sample_text = \"The patient experienced severe headaches after taking the medication.\"\n",
        "tokenized_output = tokenizer(sample_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "print(\"\\nSample Tokenization Output:\")\n",
        "print(f\"Input IDs: {tokenized_output['input_ids']}\")\n",
        "print(f\"Attention Mask: {tokenized_output['attention_mask']}\")\n",
        "print(f\"Decoded (for understanding): {tokenizer.decode(tokenized_output['input_ids'][0])}\")"
      ],
      "metadata": {
        "id": "8HBDoYGi0Tt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The label column should be named 'label' (numerical 0 or 1)\n",
        "# The text column should be named 'text' (original text, as tokenizer handles cleaning)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Truncate to the maximum input length of the model (often 512)\n",
        "    # Based on EDA, max sentence length was 742 chars. 512 tokens often covers more characters.\n",
        "    # We will use padding='max_length' to pad to the max length, and truncation=True for longer texts.\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenizer to the entire dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Rename the 'label_text' column to 'labels' and remove unnecessary columns\n",
        "# Transformers Trainer expects the target column to be named 'labels'\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"sentence_id\", \"label_text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Set the format to PyTorch tensors (important for training)\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"\\nTokenized Datasets Overview:\")\n",
        "print(tokenized_datasets)\n",
        "print(\"\\nSample of tokenized_datasets['train'][0]:\")\n",
        "print(tokenized_datasets['train'][0])"
      ],
      "metadata": {
        "id": "NEjyJTJEsrX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, AutoModelForSequenceClassification\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# load the pre-trained model for sequence classification\n",
        "# num_labels=2 for binary classification (ADE vs. Non-ADE)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# function to compute metrics\n",
        "# for evaluating model during training and after.\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = evaluate.load(\"f1\")\n",
        "    # metric_precision = evaluate.load(\"precision\")\n",
        "    # metric_recall = evaluate.load(\"recall\")\n",
        "    # metric_accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Compute F1-score (macro average is good for imbalanced datasets)\n",
        "    f1_score = metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "\n",
        "    # return other metrics as a dictionary\n",
        "    # return {\n",
        "    #     \"accuracy\": metric_accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
        "    #     \"precision\": metric_precision.compute(predictions=predictions, references=labels, average=\"macro\")['precision'],\n",
        "    #     \"recall\": metric_recall.compute(predictions=predictions, references=labels, average=\"macro\")['recall'],\n",
        "    #     \"f1\": f1_score['f1']\n",
        "    # }\n",
        "    return f1_score\n",
        "\n",
        "# output_dir: where the model checkpoints and logs will be saved\n",
        "# evaluation_strategy: 'epoch' evaluate at the end of each epoch\n",
        "# num_train_epochs: number of passes over the training data\n",
        "# weight_decay: regularization to prevent overfitting\n",
        "# load_best_model_at_end: load the model with the best validation performance\n",
        "# metric_for_best_model: the metric to monitor for early stopping/best model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16, # adjust based on GPU memory\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3, # typically 2-4 epochs are enough for fine-tuning\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\", # monitor macro F1-score on validation\n",
        "    save_strategy=\"epoch\", # checkpoints at each epoch\n",
        "    report_to=\"none\" # disable logging for simplicity\n",
        ")"
      ],
      "metadata": {
        "id": "HXX8wstQxbd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# init the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"], #'test' as our validation set\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "\n",
        "# eval the model on the test set after training\n",
        "final_results = trainer.evaluate()\n",
        "print(\"\\nFinal Evaluation Results on Test Set:\")\n",
        "print(final_results)"
      ],
      "metadata": {
        "id": "cIHZHuvPwQBG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}